# GPT Vision + RealSense Integration

Real-time scene analysis using OpenAI's GPT Vision models with Intel RealSense D435i camera for robotics applications.

## Overview

This project integrates OpenAI's GPT Vision models (GPT-4o, GPT-4o-mini, GPT-5) with the RealSense D435i RGB-D camera to provide intelligent scene understanding for robots. The system captures RGB and depth data, analyzes scenes using GPT Vision API, and provides actionable insights for navigation, object detection, and safety assessment.

## Features

- **Real-time Vision Analysis**: Process RealSense camera feeds with GPT Vision API
- **Depth Image Support**: Optional depth map visualization for enhanced spatial understanding
- **Multiple Model Support**: Choose from gpt-4o, gpt-4o-mini, or gpt-5-chat-latest
- **Korean/English Prompts**: Bilingual prompt templates (_KR versions available)
- **Customizable Prompts**: 8+ prompt templates for different robotics tasks
- **Token Cost Tracking**: Monitor and estimate API costs in real-time
- **Async Processing**: Non-blocking API calls for continuous operation
- **Comprehensive Logging**: Save images and analysis results with timestamps
- **Continuous Analysis**: No artificial rate limiting, runs as fast as API allows

## Project Structure

```
gpt-vlm/
â”œâ”€â”€ gpt_realsense_analyzer.py  # Main application
â”œâ”€â”€ config.py                   # Configuration settings
â”œâ”€â”€ prompts.py                  # Prompt templates (EN + KR)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ logs/                       # Analysis results (created at runtime)
    â”œâ”€â”€ frame_*.jpg             # Captured images
    â””â”€â”€ analysis_*.json         # GPT responses
```

## Prerequisites

### Hardware

- Intel RealSense D435i camera
- NVIDIA Jetson Orin NX (or compatible device)
- Stable internet connection

### Software

**Python Packages:**
```bash
pip3 install openai pyrealsense2 opencv-python numpy python-dotenv
```

Or use the provided requirements file:
```bash
pip3 install -r requirements.txt
```

**OpenAI API Key:**
- Get your API key from [OpenAI Platform](https://platform.openai.com/api-keys)
- Ensure you have credits or billing set up

## Setup

### 1. Configure API Key

Create a `.env` file in the project directory:

```bash
cd /home/unitree/AIM-Robotics/gpt-vlm
cp .env.example .env
nano .env
```

Add your OpenAI API key:
```
OPENAI_API_KEY=sk-your-actual-api-key-here
```

### 2. Verify RealSense Camera

Check that your RealSense camera is connected:

```bash
lsusb | grep -i intel
# Should show: 8086:0b3a Intel Corp. RealSense D435i
```

### 3. Install Dependencies

```bash
pip3 install -r requirements.txt
```

## Configuration

Edit `config.py` to customize behavior:

### Model Selection
```python
OPENAI_MODEL = "gpt-5-chat-latest"  # Options:
                                     # "gpt-4o" - High accuracy
                                     # "gpt-4o-mini" - Cost-effective
                                     # "gpt-5-chat-latest" - Latest GPT-5
```

### Depth Image Option
```python
SEND_DEPTH_IMAGE = True   # Send RGB + depth colormap (2x tokens)
                          # False = RGB only (cheaper)
```

When enabled, GPT receives:
1. **RGB image**: Standard camera view
2. **Depth colormap**: Blue=close, green=mid, red=far
3. **Instruction**: Analyze depth map to estimate object distances

### Image Quality Settings
```python
IMAGE_DETAIL = "low"      # "low" saves ~65% tokens vs "high"
JPEG_QUALITY = 75         # 0-100, compression quality
```

### Other Options
```python
MAX_TOKENS = 300          # Maximum response tokens
TEMPERATURE = 0.7         # 0.0-2.0, creativity level
SAVE_IMAGES = True        # Save analyzed frames to disk
SAVE_RESPONSES = True     # Save GPT responses to JSON
```

## Usage

### Basic Usage

Run the analyzer:

```bash
cd /home/unitree/AIM-Robotics/gpt-vlm
python3 gpt_realsense_analyzer.py
```

**Expected Output:**
```
ğŸ¥ Initializing RealSense D435i...
  - Depth stream: 640x480 @ 30fps
  - Color stream: 640x480 @ 30fps
  - Waiting for device to be ready...
  âœ“ Pipeline started
  - Warming up camera (30 frames)...
  âœ“ Camera ready

============================================================
ğŸ¥ GPT VISION + REALSENSE ANALYZER
============================================================

Configuration:
  Model:         gpt-5-chat-latest
  Detail:        low
  JPEG Quality:  75%
  Resolution:    640x480
  Depth Image:   Enabled (2x tokens)

============================================================
Press Ctrl+C to stop
============================================================

============================================================
Analysis #1 | 4523.1ms
============================================================

ğŸ“ Depth: 1.25m

ğŸ¤– GPT Analysis:

1. ë³´ì´ëŠ” ë¬¼ì²´ë“¤
   - ì˜ì 2ê°œ (ê¹Šì´ë§µ ì´ˆë¡ìƒ‰, ì•½ 1.3-1.5m)
   - íƒì ë‹¤ë¦¬ (ê¹Šì´ë§µ ë…¸ë€ìƒ‰, ì•½ 2.0m)
   - ê°€ë°© (ê¹Šì´ë§µ íŒŒë€ìƒ‰, ì•½ 0.8-0.9m)
   - ì „ì› ì¼€ì´ë¸” (ê¹Šì´ë§µ íŒŒë€ìƒ‰, ì•½ 0.8m)

2. ê³µê°„ ë°°ì¹˜
   - ì™¼ìª½: ê°€ë°©, ë°”ë‹¥ ê°€ê¹Œì›€
   - ì˜¤ë¥¸ìª½: ì˜ì, ë‹¤ë¦¬ ê³µê°„ ìˆìŒ
   - ì¤‘ì•™: ì¼€ì´ë¸”ì´ ë°”ë‹¥ì„ ê°€ë¡œì§€ë¦„

3. ì¥ì• ë¬¼ / ìœ„í—˜ ìš”ì†Œ
   - ë°”ë‹¥ì˜ ì¼€ì´ë¸”: ê±¸ë¦¼ ë° ê°ê¹€ ìœ„í—˜
   - ì˜ì ë‹¤ë¦¬: ì¶©ëŒ ê°€ëŠ¥

4. ì£¼í–‰ ì œì•ˆ í–‰ë™
   - ì „ë°© ì¼€ì´ë¸”ì„ í”¼í•˜ê¸° ìœ„í•´ ìš°ì¸¡ìœ¼ë¡œ ìš°íšŒ ì´ë™
   - ì†ë„ ì €ì† ìœ ì§€ (0.2 m/s ì´í•˜)

ğŸ“Š Tokens: 1247 in / 152 out
============================================================
```

### Available Prompts

**English Versions:**
- `SCENE_UNDERSTANDING` - General scene description (default)
- `OBJECT_DETECTION` - List objects with locations
- `OBSTACLE_DETECTION` - Identify navigation hazards
- `PERSON_TRACKING` - Detect and locate people
- `ROOM_CLASSIFICATION` - Identify room type
- `SAFETY_CHECK` - Safety assessment (SAFE/CAUTION/STOP)
- `DIRECTION_RECOMMENDATION` - Navigation direction advice
- `PATH_DESCRIPTION` - Describe navigable paths

**Korean Versions (_KR suffix):**
- `SCENE_UNDERSTANDING_KR` - ì¥ë©´ ì´í•´ (ê¸°ë³¸ê°’)
- `OBJECT_DETECTION_KR` - ë¬¼ì²´ íƒì§€
- `OBSTACLE_DETECTION_KR` - ì¥ì• ë¬¼ íƒì§€
- And all others with `_KR` suffix

**Switching Prompts:**
Edit `gpt_realsense_analyzer.py` line ~144:
```python
async def analyze_frame(self, ..., prompt_template=prompts.OBSTACLE_DETECTION_KR):
```

## Cost Estimation

### Token Usage

**RGB only (SEND_DEPTH_IMAGE = False):**
- Image: ~600 tokens
- Prompt: ~50 tokens
- Response: ~100-150 tokens
- **Total: ~750-800 tokens/analysis**

**RGB + Depth (SEND_DEPTH_IMAGE = True):**
- Images: ~1200 tokens (2 images)
- Prompt: ~100 tokens (includes depth instructions)
- Response: ~150-200 tokens
- **Total: ~1450-1500 tokens/analysis**

### Pricing

Varies by model (check [OpenAI Pricing](https://openai.com/api/pricing/)):

| Model | Input (per 1M tokens) | Output (per 1M tokens) |
|-------|----------------------|------------------------|
| gpt-4o-mini | $0.150 | $0.600 |
| gpt-4o | $2.50 | $10.00 |
| gpt-5-chat | ~$5.00 | ~$15.00 |

### Cost Examples (Continuous Analysis)

**Assuming 4-5 second API response time = ~0.2 fps effective rate**

**gpt-4o-mini + RGB only:**
- ~720 analyses/hour Ã— 800 tokens = ~0.58M tokens/hour
- Input: $0.09/hour, Output: $0.03/hour
- **Total: ~$0.12/hour (~$3/day)**

**gpt-5-chat + RGB + Depth:**
- ~720 analyses/hour Ã— 1500 tokens = ~1.08M tokens/hour
- Input: ~$5.40/hour, Output: ~$1.62/hour
- **Total: ~$7/hour (~$168/day)**

**Cost Reduction Tips:**
1. Use `gpt-4o-mini` instead of gpt-4o or gpt-5
2. Set `SEND_DEPTH_IMAGE = False` (saves 50% tokens)
3. Use `IMAGE_DETAIL = "low"` (already default)
4. Add conditional analysis (skip frames when robot is stopped)

## Performance

**Typical Analysis Time:**
- API latency: 2-5 seconds (varies by model and load)
- Continuous operation: Analyzes immediately after each completion
- Effective rate: ~0.2-0.5 fps depending on API response time

**The system runs continuously without artificial delays:**
- Frame received â†’ Analyze â†’ Complete â†’ Next frame immediately
- No fixed FPS limit, depends only on API speed

## Output Examples

### Saved Files

**RGB Image**: `logs/frame_20250105_143022_123.jpg`

**Depth Colormap** (if enabled): Saved alongside RGB

**Analysis JSON**: `logs/analysis_20250105_143022_123.json`
```json
{
  "timestamp": "20250105_143022_123",
  "analysis_count": 42,
  "success": true,
  "analysis": "1. ë³´ì´ëŠ” ë¬¼ì²´ë“¤\n- ì˜ì 2ê°œ (ê¹Šì´ë§µ ì´ˆë¡ìƒ‰, ì•½ 1.3-1.5m)...",
  "prompt": "ë¡œë´‡ ê´€ì ì—ì„œ ì´ ì¥ë©´ì„ ë¶„ì„í•˜ì„¸ìš”...",
  "depth_m": 1.25,
  "tokens": {
    "input": 1247,
    "output": 152
  }
}
```

## Troubleshooting

### Camera Not Detected

```bash
# Check USB connection
lsusb | grep -i intel

# Test RealSense
cd /home/unitree/AIM-Robotics/RealSense/examples
python3 00_check_camera.py
```

### "Device or resource busy" Error

**Cause**: Unitree's `videohub_pc4` service uses the camera

**Solution:**
```bash
# Check if videohub is running
ps aux | grep videohub

# Kill it (will auto-restart, but gives brief window)
sudo kill -9 <PID>

# Immediately run your script
python3 gpt_realsense_analyzer.py
```

**Permanent solution:**
```bash
# Rename binary to disable auto-restart
sudo mv /unitree/module/video_hub_pc4/videohub_pc4 \
        /unitree/module/video_hub_pc4/videohub_pc4.disabled

# Restore later if needed
sudo mv /unitree/module/video_hub_pc4/videohub_pc4.disabled \
        /unitree/module/video_hub_pc4/videohub_pc4
```

### API Key Issues

```
âŒ Error: OPENAI_API_KEY not found
```

**Solution:**
- Verify `.env` file exists in `/home/unitree/AIM-Robotics/gpt-vlm/`
- Check key starts with `sk-`
- Ensure no extra spaces or quotes

### Slow API Response

**If analysis takes >10 seconds:**
- Check internet connection
- Try switching to `gpt-4o-mini` (faster than gpt-5)
- Verify OpenAI API status

## Depth Map Analysis

When `SEND_DEPTH_IMAGE = True`, GPT receives depth visualization with instructions:

**Depth Colormap Scale:**
- ğŸ”µ Blue: 0.5-1.0m (close)
- ğŸŸ¢ Green: 1.0-2.0m (medium)
- ğŸŸ¡ Yellow/ğŸ”´ Red: 2.0m+ (far)

**GPT is instructed to:**
1. Look at each object in RGB image
2. Check corresponding area in depth map
3. Estimate distance based on color
4. Report as: "ì˜ì (ê¹Šì´ë§µ ì´ˆë¡ìƒ‰, ì•½ 1.3-1.5m)"

This enables spatial awareness without complex depth processing.

## Comparison with YOLO

| Feature | GPT Vision | YOLOv8 |
|---------|------------|--------|
| **Speed** | ~0.2-0.5 fps | ~30 fps |
| **Cost** | $3-168/day | Free |
| **Understanding** | Scene reasoning, distance estimation | Bounding boxes only |
| **Output** | Natural language | Class + coordinates |
| **Depth** | Analyzes depth map | No depth awareness |
| **Use Case** | High-level decisions | Real-time tracking |

**Recommendation**: Use both together:
- **YOLO**: Real-time object tracking (30 fps, local)
- **GPT Vision**: Scene understanding and planning (0.5 fps, cloud)

## Advanced Usage

### Custom Prompts

Add to `prompts.py`:

```python
MY_CUSTOM_PROMPT_KR = """ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸.
ì¤‘ì•™ ê±°ë¦¬: {depth_m}m
[ì§€ì‹œì‚¬í•­]"""

# Use in analyzer:
# Line ~144: prompt_template=prompts.MY_CUSTOM_PROMPT_KR
```

### Robot Control Integration

```python
# After receiving analysis result:
analysis_text = result["analysis"]

if "STOP" in analysis_text or "ì •ì§€" in analysis_text:
    robot.stop()
elif "FORWARD" in analysis_text or "ì „ì§„" in analysis_text:
    robot.move_forward()
```

### Conditional Analysis

Add logic to skip frames:

```python
# Only analyze if robot is moving
if robot.is_moving():
    result = await self.analyze_frame(...)
```

## Model Comparison

**gpt-4o-mini:**
- âœ… Fast (~2-3s response)
- âœ… Very cheap ($3/day continuous)
- âš ï¸ Good but not exceptional understanding

**gpt-4o:**
- âœ… Excellent understanding
- âš ï¸ Moderate cost (~$50/day continuous)
- âš ï¸ Slower (~4-5s response)

**gpt-5-chat-latest:**
- âœ… Best understanding and reasoning
- âœ… Handles complex spatial reasoning
- âŒ Expensive (~$168/day continuous)
- âš ï¸ Can be slow (~5-7s response)

## References

**Related Projects:**
- YOLOv8 + RealSense: `/home/unitree/AIM-Robotics/YOLOv8n/`
- RealSense examples: `/home/unitree/AIM-Robotics/RealSense/`
- SLAM system: `/home/unitree/AIM-Robotics/SLAM/`

**Documentation:**
- [OpenAI Vision API](https://platform.openai.com/docs/guides/vision)
- [RealSense D435i](https://www.intelrealsense.com/depth-camera-d435i/)
- [OpenAI Pricing](https://openai.com/api/pricing/)

---

Made with ğŸ’¡ by AIM Robotics
